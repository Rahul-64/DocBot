{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Document Ingestion and QA Notebook (Pinecone Edition)\n",
                "\n",
                "This notebook loads PDFs, chunks the text, creates embeddings, and uploads them to a **Pinecone Vector Database**. It then uses an LLM to answer questions based on the persistent cloud memory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install ipywidgets to fix the TqdmWarning if needed\n",
                "# !pip install ipywidgets pinecone-client langchain-pinecone"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_community.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from langchain_pinecone import PineconeVectorStore\n",
                "from pinecone import Pinecone\n",
                "import torch\n",
                "import os\n",
                "from dotenv import load_dotenv, find_dotenv\n",
                "\n",
                "load_dotenv(find_dotenv())\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                " # Set to False to skip loading and uploading if index already exists"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Load PDF Files (Conditional)\n",
                "Load all PDF files from the `data/` directory using `PyMuPDFLoader`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Skipping PDF loading (Using existing Pinecone index)\n"
                    ]
                }
            ],
            "source": [
                "DATA_PATH = \"data/\"\n",
                "\n",
                "# --- AUTO-DETECT if Pinecone index already has data ---\n",
                "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
                "PINECONE_INDEX_NAME = \"docbot-index\"\n",
                "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
                "index = pc.Index(PINECONE_INDEX_NAME)\n",
                "stats = index.describe_index_stats()\n",
                "vector_count = stats.get(\"total_vector_count\", 0)\n",
                "UPDATE_VECTOR_DB = vector_count == 0  # Auto: True if empty, False if already has data\n",
                "\n",
                "if UPDATE_VECTOR_DB:\n",
                "    def load_pdf_files(data):\n",
                "        loader = DirectoryLoader(\n",
                "            data,\n",
                "            glob=\"**/*.pdf\",\n",
                "            loader_cls=PyMuPDFLoader,\n",
                "            use_multithreading=True\n",
                "        )\n",
                "        return loader.load()\n",
                "\n",
                "    documents = load_pdf_files(DATA_PATH)\n",
                "    print(f\"Loaded {len(documents)} document pages\")\n",
                "else:\n",
                "    print(\"Skipping PDF loading (Using existing Pinecone index)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Create Text Chunks (Conditional)\n",
                "Split the loaded documents into smaller chunks for processing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Skipping chunk creation\n"
                    ]
                }
            ],
            "source": [
                "if UPDATE_VECTOR_DB:\n",
                "    def create_chunks(extracted_data):\n",
                "        text_splitter = RecursiveCharacterTextSplitter(\n",
                "            chunk_size=500,\n",
                "            chunk_overlap=50\n",
                "        )\n",
                "        text_chunks = text_splitter.split_documents(extracted_data)\n",
                "        return text_chunks\n",
                "\n",
                "    text_chunks = create_chunks(documents)\n",
                "    print(f\"Created {len(text_chunks)} text chunks\")\n",
                "else:\n",
                "    print(\"Skipping chunk creation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Initialize Embedding Model (Optimized for GPU)\n",
                "Load the HuggingFace embedding model (`sentence-transformers/all-MiniLM-L6-v2`) and set it to use CUDA if available."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "def get_embedding_model():\n",
                "    # Check if CUDA (GPU) is available\n",
                "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "    print(f\"Using device: {device}\")\n",
                "    \n",
                "    embedding_model = HuggingFaceEmbeddings(\n",
                "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
                "        model_kwargs={\"device\": device}\n",
                "    )\n",
                "    return embedding_model\n",
                "\n",
                "embedding_model = get_embedding_model()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Upload to Pinecone (Conditional)\n",
                "Push the generated vectors to your Pinecone index only if `UPDATE_VECTOR_DB` is True.\n",
                "**Pre-requisite:** Ensure `PINECONE_API_KEY` is in your `.env` file and you have created an index named `docbot-index`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Skipping upload (Using existing Pinecone index)\n"
                    ]
                }
            ],
            "source": [
                "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
                "PINECONE_INDEX_NAME = \"docbot-index\"\n",
                "\n",
                "if not PINECONE_API_KEY:\n",
                "    raise ValueError(\"PINECONE_API_KEY not found in .env file\")\n",
                "\n",
                "if UPDATE_VECTOR_DB:\n",
                "    print(f\"Uploading vectors to Pinecone index '{PINECONE_INDEX_NAME}'...\")\n",
                "    \n",
                "    # Batch upload to Pinecone\n",
                "    if text_chunks:\n",
                "        docsearch = PineconeVectorStore.from_documents(\n",
                "            documents=text_chunks,\n",
                "            embedding=embedding_model,\n",
                "            index_name=PINECONE_INDEX_NAME,\n",
                "            pinecone_api_key=PINECONE_API_KEY\n",
                "        )\n",
                "        print(\"Successfully uploaded vectors to Pinecone!\")\n",
                "    else:\n",
                "        print(\"No text chunks to upload.\")\n",
                "else:\n",
                "    print(\"Skipping upload (Using existing Pinecone index)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Setup LLM (Mistral-7B-Instruct)\n",
                "Set up the `HuggingFaceEndpoint` to use the Mistral-7B-Instruct model. Ensure `HF_TOKEN` must be set in your `.env` file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "LLM loaded successfully\n"
                    ]
                }
            ],
            "source": [
                "from langchain_groq import ChatGroq\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain.chains import RetrievalQA\n",
                "\n",
                "llm = ChatGroq(\n",
                "    model_name=\"llama-3.3-70b-versatile\",\n",
                "    temperature=0.0,\n",
                "    groq_api_key=os.environ[\"GROQ_API_KEY\"],\n",
                ")\n",
                "print(\"LLM loaded successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Setup QA Chain\n",
                "Define the custom prompt template and creating the RetrievalQA chain using the Pinecone vector store."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "QA Chain initialized\n"
                    ]
                }
            ],
            "source": [
                "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
                "Use the pieces of information provided in the context to answer user's question.\n",
                "If you dont know the answer, then search on your parameter but if user asked who is mehak then say Mehak is \n",
                "girlfriend of Rahul , Rahul loves her so much\n",
                "\n",
                "Context: {context}\n",
                "Question: {question}\n",
                "\n",
                "Start the answer directly. No small talk please.\n",
                "\"\"\"\n",
                "\n",
                "def set_custom_prompt(custom_prompt_template):\n",
                "    prompt = PromptTemplate(template=custom_prompt_template, input_variables=[\"context\", \"question\"])\n",
                "    return prompt\n",
                "\n",
                "# Ensure we use the existing index (in case we didn't just upload it)\n",
                "docsearch = PineconeVectorStore.from_existing_index(\n",
                "    index_name=PINECONE_INDEX_NAME,\n",
                "    embedding=embedding_model\n",
                ")\n",
                "\n",
                "qa_chain = RetrievalQA.from_chain_type(\n",
                "    llm=llm,\n",
                "    chain_type=\"stuff\",\n",
                "    retriever=docsearch.as_retriever(search_kwargs={'k': 3}),\n",
                "    return_source_documents=True,\n",
                "    chain_type_kwargs={'prompt': set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}\n",
                ")\n",
                "\n",
                "print(\"QA Chain initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. Run Query\n",
                "Run a query against the document bot."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RESULT:  Rahul is not mentioned in the context, but according to the parameter, Rahul loves Mehak so much, she is his girlfriend.\n"
                    ]
                }
            ],
            "source": [
                "user_query = input(\"Write Query Here: \")\n",
                "if user_query:\n",
                "    response = qa_chain.invoke({'query': user_query})\n",
                "    print(\"RESULT: \", response[\"result\"])\n",
                "    # print(\"SOURCE DOCUMENTS: \", response[\"source_documents\"])\n",
                "else:\n",
                "    print(\"No query entered.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
